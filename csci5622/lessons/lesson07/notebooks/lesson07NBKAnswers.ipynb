{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: The Kernel Trick and Parameter Tuning  \n",
    "***\n",
    "\n",
    "<img src=\"figs/cogs.jpg\",width=1100,height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reminder: Scroll down to the bottom and shift-enter all of the Helper Functions*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 1: Polynomial Kernels \n",
    "***\n",
    "\n",
    "Consider the following labeled data (with red and blue indicating the positive and negative labels, respectively)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcwAAAHMCAYAAABY25iGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFs5JREFUeJzt3XuQnXWZ4PHnfc9JJ+l0rsTWRDpEUXcgQYlBF0cU2YJS\nR6yRCVldq1xKarGUceAfLRd1tyymtlaprUqpTFmVXZKyKCxnmBJWRsetTcnNZRVFMizMBYGES0BC\n50Z3p2/nvO/+0SZFJCRPx3Sf7uTz+S/nPd39pKu7v+f33k5R13UAAMdWdnoAAJgNBBMAEgQTABIE\nEwASBBMAEprH2e4UWgBON8XRHrTCBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATB\nBIAEwQSABMEEgATBBIAEwQSABMEEgITjvR8mME1uvfXW2Lp1awwPD0dvb2/ccsstMTw8HNdee20M\nDw/H4sWL4+abb44VK1Z0elQ4LRV1fcz3iPYG0jANNm3aFPv27Yuvfe1rUZZlbNiwIfbv3x/d3d2x\nefPm2LFjR2zYsCE+8YlPxKZNmzo9LpzqvIE0zEQ7duyIhx9+OG688cYoy4lfybVr18bdd98dGzdu\njBUrVsTtt98eu3fvjvPPP7/D08LpywoTOuzGG2+MDRs2xJo1aw4/tmHDhvjhD38Ye/bsiUWLFsXo\n6Gg8/vjjcd5550VExMjISHzjG9+I3bt3x2OPPRZveMMb4qabbopVq1Z16r8Bp5KjrjAFE2ag3t7e\nWL16dTz44INH3f6Vr3wlPve5z8WZZ54ZERFXXXVV/PSnP41HHnkkli5dOp2jwqnILlmYDbZv3x79\n/f1xySWXHHX76OhofOtb34otW7YcfuyrX/1q7Nq1K7Zu3TpdY8JpRzBhhtm2bVsURfGawWy327F8\n+fIYHh4+/NhZZ50VERFPPPHEtMwIpyOXlUCHvfjiizE2NhZ9fX0RMRHMsizjoosuOuJ5V1xxRdxx\nxx3R3d0dO3bsOGLbU089FRERZ5999vQMDachwYQO2rdvX5xzzjlRFEXs2bMn9u7dG/fee2/09fVF\nT0/P4efdeeedceGFF77m57ntttuit7c3rr766ukYG05LdslCB+3cuTMGBwfjs5/9bFRVFddff31c\nc801sXv37ujv74+IiHvuuSe2bNkSX/jCF476OZ555pn4zne+E1u3bnXCD0whK0zooHXr1sUNN9wQ\nDzzwQFx88cVx3XXXxcaNG6Ovry8uvfTS6O7ujjVr1sRtt90WjUbjVR8/Pj4en/70p2Pz5s3x4Q9/\nuAP/Azh9uKwEZrHPf/7z8dGPfjQ++MEPRkTEk08+6Tgm/OFcVgKnkm9+85vxkY985HAsW61WfP/7\n3+/wVHDqsksWZpixsbFotVrR1dUVzebRf0XvuOOOuOuuu+Kyyy6LRx55JCIidu3adcwTg4A/jF2y\nMIPUdR27du2KgYGBOOOMM6K3t/dVz9m7d2+sWrXqiOswD/nFL34RF1xwwXSMCqeyo+6StcKEGWRw\ncDCGhobiwIED0dXVFUuWLImurq4jnrNs2bIYHBzs0IRw+nIME2aIuq7jwIEDMTg4GM1mM4aGhmL/\n/v2dHgv4HcGEGeLQ6rKqqli6dGkMDw/HwMBAjI2NdXo0IAQTZoRXri57enqi0WjE/PnzrTJhBhFM\nmAFeubqcN29eREQsWLDAKhNmEMGEDvv9Y5cvv/xy7NmzJw4ePOhYJswgggkdNjg4GKOjo9FsNqMs\nJ34ly7KMqqqi2WxGXdcxNDRklQkd5jpM6LD9+/dHq9U6/O/BwcFotVrRbDaPeMeSefPmHfFvYMq4\nDhNmoiVLlhzx70PXWHZ1dcXy5cs7MRJwFHbJAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJA\ngmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCC\nYAJAgmACQIJgAkCCYAJAgmACQEKz0wPMVuPDI/HMr38dux59LMaHh6PZ1RXLz35zvPnCfx3zFy/u\n9HgA8fRLA/GTh5+NnbsHolVVsWTB3Lhkzcp491t7o9mwXpqsoq7rY20/5sbTUbvVikfu+rvY+ctf\nRtQRVbsdRVFEHRFFUURExOvf9rZYv/HKmLewp7PDMivt3LkzWq1WdHd3x8qVKzs9DrPQ0y8NxLd/\n/Gg80z8Y7aqOIiKiiKjrOuY0ymg2yvj4e98Sl69fdfjvFkc46jdFMCehPT4e92/+H7H32WejKIoo\nyle/QqvrOup2O+YuXBj/5i/+3GqTSRNM/hD/8vz++Npf/yrGWu2Y0yiPGsR2VUVdR1z69jPjM5ed\nI5qvdtRviDX5JPzqr/9mIpZledRYRkysMstmM0YGBuL+zf89qnZ7mqcETld7BkbiL29/KMZbVXQ1\nG68ZwkZZRqMsYtsjz8XfPfTMNE85ewlm0uCePfH8o49NxDLxaqxsNGJo37544Z/+aRqmA4j40UNP\nx8hYO+Y0j/+nvSiKKIqIv3ngiWi1q2mYbvYTzKSnHvi/Udd1etdFURRRtdrx+D33TfFkABFjrXb8\nr+3PRVnmd682yjLGWlU8+MTuKZzs1CGYSc88/HDEJPfzl81m7Hvu2Rg7eHCKpgKY8M+79kdV19GY\nRDAjIlrtKu5+9PkpmurUIphJ48Mjr3nc8rUURRFl2YjRIcEEptbA8Hgc5yTOoyqKIvYPjU7BRKce\nwUyabCwPqSOibDZO7jAAv6dRFid2tmsdMcc1mSm+S0k9y8+Y9BmvdVVFUUTM61k4RVMBTFi5tDva\nVT3pVWYddZz1On+jMgQz6a3vf3+Uk3wVVldVnLV+fTTmuKESMLVWvW5hvPGMBdFq54NZ13U0yjI+\nvK5vCic7dQhm0hvPWxtl2UivMuu6jqIs4+z3/vEUTwYw4Yp3vynKskivMsfbVaxa3hOrrDBTBDOp\n2dUV6//txigioqqOfc1SXddRV1W85X0XxaLXv356BgROe+/9o9fHuX1Lo5XYNXvo5gZ/8Sdrp2m6\n2U8wJ+HMt58X667cEEVM3Cav/r1w1nUd7fHxiLqOs//4PbH2Qx/szKDAaalRlvEfP3Z+rF21LKp6\nIoq/H852Vcd4q4p5XY342scvcPxyEtxL9gQceOGFePze++K5f3hk4uzZuo4oiqjb7Vh+9pvjX33g\nA9H71rd0ekxmKfeS5Q/Vrqr4P//8Ytz54I54bs9QNH63m3biUrciPnR+X/zJO1fFGQvndXrUmcrN\n10+2seHh2Pv00zE+MhrNrq5YvGJFdC9d0umxmOUEk5PpmZcG4oX9B6PVrmPR/DnxR2cudRnJ8Qkm\nzAaCCR3n3UoA4EQJJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgm\nACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYA\nJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAk\nCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQI\nJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgm\nACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYA\nJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAk\nCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQI\nJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgmACQIJgAkCCYAJAgm\nACQIJgAkCCYAJAgmACQIJgAkCCYAJDQ7PQAATNatt94aW7dujeHh4ejt7Y1bbrklhoeH49prr43h\n4eFYvHhx3HzzzbFixYqT9jUFE4BZZdOmTbFv377Ytm1blGUZGzZsiI9//OPR3d0dmzdvjh07dsSG\nDRvipptuik2bNp20r2uXLADTbnBwMMbGxib9cTt27IiHH344brzxxijLiYStXbs27r777ti4cWOs\nWLEibr/99ti9e3ecf/75J3VmwQRg2o2MjMSuXbti9+7dkwrnrbfeGl/60peOeOzRRx+NRqMRH/vY\nxyIi4utf/3ps3749rrrqqiOed//998e55557wjMXdV0fa/sxNwIn386dO6PVakV3d3esXLmy0+PA\nlOjv749du3ZFRERPT08sXLgwlixZEl1dXZP+XL29vbF69ep48MEHj7r9e9/7Xmzbti0GBgbiBz/4\nQbTb7eN9yuJoD1phAtAxrVYr+vv744UXXohnn3120ivO7du3R39/f1xyySWv+ZxPfvKTsWXLlrj8\n8sv/oFln1Uk/w8PDMTQ0dEL7vWG26O/vj3a7HUNDQ50eBabMob/jXV1dsWTJkhgaGor+/v4YHByM\ngYGBWLhwYSxbtiyazWNnatu2bVEUxTGDebLMqmAODQ3FgQMH4ji7kWFWGx0dPbzL6ODBgx2eBqZO\nVVXRarVi4cKFsWjRoliwYEHs27cvXnrppRgbG4uenp5XBfPFF1+MsbGx6Ovri4g4fKbsRRdddMTz\nrrjiirjjjjtO6ryzKphz586NuXPnRqvV6vQoMGUajUZERDSbzeO+uobZrCzLKMsy6rqOkZGRGBwc\njLIsY+nSpbFgwYJX/fzv27cvzjnnnCiKIvbs2RN79+6Ne++9N/r6+qKnp+fw8+6888648MILT/q8\ns+q3ceHChbFw4cJOjwFTzkk/nOr6+/vjwIEDUVVV9Pf3R1mWh1eZixcvjp6eniiKI8+92blzZwwO\nDsYXv/jFqKoqrr/++rjmmmtiy5Yt0d/fH8uXL4977rkntmzZctJXlxGzLJgAnFqqqjpuKA9Zt25d\n3HDDDfHAAw/ExRdfHNddd11s3Lgx+vr64tJLL43u7u5Ys2ZN3HbbbYf31JxMLiuBGcZlJZwODq0w\n586de9xQnizf/e534+qrrz7hy0qsMAGYds1mM3p7e6cllCeLYAIw7ZYsWTLtXzOxsjwmNy4A4JT2\n4x//OK688sr48pe/HBER69evj0996lMxMDAwqc/jGCbMMI5hQse5NR4AnCjBBIAEwQSABMEEgATB\nBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEE\ngATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSA\nBMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAE\nwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATB\nBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEE\ngATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSA\nBMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAE\nwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATB\nBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSABMEEgATBBIAEwQSAhGan\nB5jN6rqO/UNjcXC0FXPnNGJpT1c0Sq9BgJmjPd6KkYGXo2pXMXdBd3R1d3d6pFlLME/AwdFW3PeP\nz8edD+6MPQMj0SjLqOo65s1pxEfWr4rL3nFmLOuZ1+kxgdPYy799MZ544IF45lcPRR0RRURUVTuW\nvvHMeNsHLo4V554TZaPR6TFnlaKu62NtP+bG09FvXjgQf3n7QzE63o52XUezLKIoioiIaFdV1HVE\nWRbx5x9aE+8/d2WHp2U22rlzZ7Rareju7o6VK/0MMTl1VcWjf/+TeOJnP4u6XUXRaETxuz1fdV1H\n1WpF2WjEgmVL432fuSbmL17c4YlnpOJoD9p/OAlP/vbl+M/f/2UcHG1FWRYxp1EejmVERKMso9ko\no67r+PbfPxb3/ePzHZwWOB09cteP4jf3/yyiKKOcM+dwLCMiiqKIxpw5EUURA/174qff/qsYGRjs\n4LSzi2Amtas6/usPfh1jrXbMaR7729Yoyyiijr/6yWOxd3BkmiYETncv/uY38dTPfx5FeeSL+d9X\nFEU0ms0YHRiIX//t307jhLObYCZt39kfAyPj0dXM7fNvlGVUVR3/+x+em+LJACY8fs+9UbXbx4zl\nKxWNRvz2Xx6P4QMHpniyU4NgJv3PB3fGeKua1McURcSPfv1MtKvJfRzAZB3ctz/6n3wqymb+XM5D\nYd3x819M1VinFMFMevK3L8ecxuS+XY2yjJGxduwfGpuiqQAm7H/++YkTfJKry0Pquo6XnnpqiqY6\ntQhm0lirHZP8OYyIiLIoYmS8ffIHAniF9vhYxLGvejiqIiJaY17UZwhm0ryuZlST/Fms6zraVRXd\nXS53BabWnHnz4kRe1dd1HV3z50/BRKcewUx655vOmPSxyFZVx+sWzY8lC7qmaCqACcvOOivqqop6\nkn+nykYjVq5dO0VTnVoEM+nyC1ZHo5y4xjKrURTxp+9ePeljCgCT1TV/fpz5jrdH1c4fAqqrKqKI\nWLVu3RROduoQzKS3vGFRnPW6nmgl98uOt6qYO6cR7ztnxRRPBjDhbRe/P8pGI6rEKrOu66jrOla/\n610xZ75beWYIZlJRFPHlP3tnLO7uivF2dcyV5qGbG/ynjeuje67jl8D0WLxiRazb8GdR1PUxV5p1\nXUddVbGsry/e/tHLp3HC2U0wJ2Fpz9z4b//+PfHm3kVR1xNhbFcTr9Kqqo7R8XbUdR1LF8yN//LJ\nd8dbV7hHIzC9Vl+wPt71yX8XjWZz4sTDVmvi2ObvIlq1WhFVFW88b2287zP/IRqTuG7zdOfm6yeg\nrut44rcvx48eejoeeqo/RsfbMadRxltWLIo/fdeb4h2rz4hG6bglJ8bN1zkZWmNj8fz/ezQev+++\nGNqzJ6p2FXPmz4tV73xnvPk9F0bPGWd0esSZ7Kh/wAUTZhjBhI7zbiUAcKIEEwASBBMAEgQTABIE\nEwASBBMAEgQTABIEEwASBBMAEgQTABIEEwASBBMAEgQTABIEEwASBBMAEgQTABIEEwASBBMAEgQT\nABIEEwASBBMAEgQTABIEEwASBBMAEgQTABIEEwASBBMAEgQTABIEE2aYZrMZRVFEV1dXp0cBXqHZ\n6QGAIy1evDhGR0djwYIFnR4FeIWirutjbT/mRgA4BRVHe9AuWQBIEEwASBBMAEgQTABIEEwASBBM\nAEgQTABIEEwASBBMAEgQTABIEEwASBBMAEgQTABIEEwASBBMAEgQTABIEEwASBBMAEgQTABIEEwA\nSBBMAEgQTABIEEwASBBMAEgQTABIEEwASBBMAEgQTABIEEwASBBMAEgQTABIEEwASBBMAEgQTABI\nEEwASGgeZ3sxLVMAwAxnhQkACYIJAAmCCQAJggkACYIJAAmCCQAJ/x87Mf92N4QFzgAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f05d7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prob1plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Is this set of training data linearly separable in the traditional sense? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Clearly not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Consider applying the quadratic kernel \n",
    "\n",
    "$$\n",
    "K({\\bf x}, {\\bf z}) = \\left({\\bf x}^T{\\bf z} + 1\\right)^2\n",
    "$$\n",
    "\n",
    "What does the derived feature vector, $\\phi({\\bf x})$, associated with this kernel look like?  You can do this by expanding the $K({\\bf x}, {\\bf z})$ in terms of the features of ${\\bf x}$ and ${\\bf z}$ and then rearranging things so it looks like the dot product $\\phi({\\bf x})^T\\phi({\\bf z})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: We have \n",
    "\n",
    "\\begin{array}{rcl}\n",
    "K({\\bf x}, {\\bf y}) &=& (x_1z_1 + x_2z_2 + 1)^2  \\\\\n",
    " &=& (x_1x_1)(z_1z_1) + 2(x_1x_2)(z_1z_2) + (x_2x_2)(z_2z_2) +2x_1z_1 + 2x_2z_2 + 1 \\\\\n",
    " &=& \\phi({\\bf x})^T\\phi({\\bf z})\n",
    "\\end{array}\n",
    "\n",
    "where $\\phi({\\bf x}) = \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "x_1x_1 \\\\\n",
    "\\sqrt{2}x_1x_2\\\\\n",
    "x_2x_2 \\\\\n",
    "\\sqrt{2}x_1 \\\\\n",
    "\\sqrt{2}x_2 \\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How many features are there in the higher dimensional space? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**:  In this configuration there are 6 features in the derived feature space.  However, the last feature is constant and doesn't depend on the data.  So we could also say there are 5 useful features in the derived space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Can you see a pair of features in the derived feature space that would allow the data to linearly separated? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**:  Plotting the four points in the $\\sqrt{2}x_1$ vs. $\\sqrt{2}x_1x_2$ plane we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdYAAAHMCAYAAABoV4mnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nd/wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112560850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prob1sol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the data is easily linearly separable in this space.  In fact, from the original data you can tell that the training data can be linearly separated by considering **only** the sign on the derived $\\sqrt{2}x_1x_2$ feature.  There for a derived weight vector in feature space that would define a separating hyperplane would simply be \n",
    "\n",
    "$$\n",
    "{\\bf w} = \\left[\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Note that this is probably not the optimal linear hyperplane defined by SVM with the polynomial kernel but it is **a** separating hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 2: Parameter Tuning, Accuracy, and Cross-Validation \n",
    "***\n",
    "\n",
    "Any support vector machine classifier will have at least one parameter that needs to be tuned based on the training data.  The guaranteed parameter is the $C$ that appears in front of the slack variable term in the primal objective function, i.e. \n",
    "\n",
    "$$\n",
    "\\min_{{\\bf w}, b, {\\bf \\xi}} \\frac{1}{2}\\|{\\bf w}\\|^2 + C \\sum_{i=1}^m \\xi_i\n",
    "$$\n",
    "\n",
    "If you use a kernel fancier than the linear kernel (i.e. the traditional non-kernelized SVM) then you will likely have other parameters as well. For instance in the polynomial kernel $K({\\bf x}, {\\bf z}) = ({\\bf x}^T{\\bf z} + c)^p$ you have to select the shift $c$ and the polynomial degree $p$.  Similarly the rbf kernel\n",
    "\n",
    "$$\n",
    "K({\\bf x}, {\\bf z}) = \\exp\\left[-\\gamma\\|{\\bf x} - {\\bf z}\\|^2\\right]\n",
    "$$\n",
    "\n",
    "has one tuning parameter, namely $\\gamma$, which controls how fast the similarity measure drops off with distance between ${\\bf x}$ and ${\\bf z}$. \n",
    "\n",
    "For our examples we'll consider the rbf kernel, which gives us two parameters to tune, namely $C$ and $\\gamma$. \n",
    "\n",
    "Consider the following two dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain, ytrain = prob2GenData(N=300, seed=1235)\n",
    "plotData(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains an SVM model with an rbf kernel.  Play around with the parameters $C$ and $\\gamma$ and see how this affects the decision and support vector boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"rbf\", C=10, gamma=1)\n",
    "plotSVM(Xtrain, ytrain, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can decide what good parameters are, we have to know what *good* means.  One idea would be to partition your data into a training set and a **validation** set, train your SVM model on the training data, and then compute the accuracy of the model on the validation set.  Accuracy is typically computed as \n",
    "\n",
    "$$\n",
    "\\texttt{score} = \\frac{\\texttt{# correctly labeled points}}{\\texttt{# validation points}}\n",
    "$$\n",
    "\n",
    "We can easily compute this accuracy on the training data using our trained model as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Accuracy on training data = \", clf.score(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that's pretty good!  Go back and play with the parameters in the SVM classifier and see what happens to the accuracy.  Did you hone in on a good set of parameters?  \n",
    "\n",
    "OK.  So maybe you found a good set of parameters that yields a good accuracy on the training set.  But is this really what we're after?  Remember, our goal is to take our model and make predictions on future unseen data.  If we tweak our parameters with the accuracy on the training set in mind, it's likely that we will **overfit** the training data, and our model may not generalize to unseen data.  Let's generate some new data from the same distribution and see how our trained model performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xvalid, yvalid = prob2GenData(N=100, seed=1235)\n",
    "print \"Accuracy on validation data = \", clf.score(Xvalid, yvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that's not too bad.  It's definite a lower accuracy score than on the training data, but we kind of expect this.  Go back and and tune the parameters some more with the goal of increasing the accuracy on the validation data instead.\n",
    "\n",
    "Were you able to do better?  If so, **COOL**!  \n",
    "\n",
    "**Q**: After parameter tuning we managed to get a pretty good accuracy on the validation data.  But can we really expect a similar accuracy on data that we'll see in the future? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**A**:  NO!  Although it seems like our model is completely independent from the validation data (after all, we trained our model on the training data) the fact that we then went back and did parameter tuning on on the validation set is **also a form of model training**!  In other words, the validation set has in some sense become part of the training data.  To truly predict how well your model will do on unseen data, you need to find a **NEW** set of data, which we usually call the **test** data, and test your model on that.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's get a new set of completely unseen data and test the SVM model on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtest, ytest = prob2GenData(N=100, seed=1241)\n",
    "print \"Accuracy on test data = \", clf.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, because we've been using simulated data we've been able to generate new data as needed on the fly.  In reality you're usually given a set of labeled training data and that's it.  You can then partition your labeled data into training, validation, and test data and run your experiments. Typically the size of the split is **60%** training data, **20%** validation data, and **20%** test data. However, other splits are common as well. \n",
    "\n",
    "What do you do if you have such little data that you can't afford to set aside 40% of it for validation and testing?  One popular solution is something called **cross-validation**.  In this case you would still set aside 20% of the data as a final test set, but you get to keep the other 20% in the training set.  A popular form of cross-validation called **K-Folds** cross-validation works as follows \n",
    "\n",
    "- Divide your training data into $K$ \"folds\" equal size sets\n",
    "- Loop over each of the folds and treat it as a hold-out set, while training on the other $K-$ folds \n",
    "- Compute the accuracy score for each of the $K-1$ models on the held-out data \n",
    "- Estimate the accuracy for your choice of parameters as the mean of the accuracy of the $K-1$ tests\n",
    "\n",
    "Let's see this in action for the example above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=4, gamma=3)\n",
    "scores = cross_validation.cross_val_score(clf, Xtrain, ytrain, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either look at the individual accuracies across each of the folds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we generally want though is an overall accuracy score for our model with our chosen set of parameter, which we can get just by taking the mean: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean Accuracy in Cross-Validation = \", scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 3: Automating the Parameter Search \n",
    "***\n",
    "\n",
    "On the previous problem we were able to choose some OK parameters just by hand-tuning.  But in real life (where time is money) it would be better to do something a little more automated.  One common thing to do is a **grid-search** over a predefined range of the parameters.  In this case you will loop over all possible combinations of parameters, estimate the accuracy of your model (using, say, K-Folds cross-validation) and then choose the parameter combination that produces the highest validation accuracy. \n",
    "\n",
    "Below is an experiment where we search over a logarithmic range between $10^{-2}$ and $10^{10}$ for $C$ and a range between $10^{-9}$ and $10^{3}$ for $\\gamma$.  For the accuracy measure we use K-Folds CV with $K=3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crng = np.logspace(-2,10,13)\n",
    "grng = np.logspace(-9,3,13)\n",
    "param_grid = dict(gamma=grng), C=crng)\n",
    "grid = GridSearchCV(SVC(kernel=\"rbf\"), param_grid=param_grid, cv=3)\n",
    "grid.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function plots a heatmap that shows the validation accuracy for each parameter combination.  Which one looks the best to you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotGrid(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearchSV object stores (among other things) the best combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the accuracy score achieved by the best set of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also stores the classifier with the best parameters, which we can pass into our plotSVM function to see the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotSVM(Xtrain, ytrain, grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation \n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from matplotlib.colors import Normalize\n",
    "%matplotlib inline\n",
    "\n",
    "def prob1plot():\n",
    "    X = np.array([[1,1], [1,-1], [-1,1], [-1,-1]])\n",
    "    y = np.array([1, -1, -1, 1])\n",
    "    mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "    msize=200\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    colors = [mycolors[\"blue\"] if yi==1 else mycolors[\"red\"] for yi in y]\n",
    "    plt.scatter(X[:,0],X[:,1], marker='o', color=colors, s=msize, alpha=0.90)\n",
    "    plt.arrow(-1.25,0,2.5,0, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    plt.arrow(0,-1.25,0,2.5, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    ax.text(1.32, -.025, r\"$x_1$\", fontsize=24)\n",
    "    ax.text(-.05, 1.32, r\"$x_2$\", fontsize=24)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.xticks([], fontsize=16)\n",
    "    plt.yticks([], fontsize=16)\n",
    "    \n",
    "def prob1sol():\n",
    "    X = np.array([[1,1], [1,-1], [-1,1], [-1,-1]])\n",
    "    y = np.array([1, -1, -1, 1])\n",
    "    \n",
    "    P = np.zeros((X.shape[0], 6))\n",
    "    P[:,0] = X[:,0] * X[:,0]\n",
    "    P[:,1] = np.sqrt(2)*X[:,0] * X[:,1]\n",
    "    P[:,2] = X[:,1] * X[:,1]\n",
    "    P[:,3] = np.sqrt(2)*X[:,0]\n",
    "    P[:,4] = np.sqrt(2)*X[:,1]\n",
    "    P[:,5] = 1 + 0.0 * X[:,1]\n",
    "    \n",
    "    mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "    msize=200\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    colors = [mycolors[\"blue\"] if yi==1 else mycolors[\"red\"] for yi in y]\n",
    "    plt.scatter(P[:,3],P[:,1], marker='o', color=colors, s=msize, alpha=0.90)\n",
    "    plt.arrow(-1.5,0,3.0,0, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    plt.arrow(0,-1.5,0,3.0, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    ax.text(1.55, -.05, r\"$\\sqrt{2}x_1$\", fontsize=24)\n",
    "    ax.text(-.5, 1.65, r\"$\\sqrt{2}x_1x_2$\", fontsize=24)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.xticks([], fontsize=16)\n",
    "    plt.yticks([], fontsize=16) \n",
    "    \n",
    "\n",
    "def prob2GenData(N=100, seed=1235):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X = np.random.uniform(-1,1,(N,2))\n",
    "    y = np.array([1 if y-x > 0 else -1 for (x,y) in zip(X[:,0]**2 * np.sin(2*np.pi*X[:,0]), X[:,1])])\n",
    "    X = X + np.random.normal(0,.1,(N,2))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def plotData(X, y):\n",
    "    mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "    msize=100\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    ax = fig.add_subplot(111)\n",
    "    colors = [mycolors[\"blue\"] if yi==1 else mycolors[\"red\"] for yi in y]\n",
    "    plt.scatter(X[:,0],X[:,1], marker='o', color=colors, s=msize, alpha=0.25)\n",
    "    plt.arrow(-1.25,0,2.5,0, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    plt.arrow(0,-1.25,0,2.5, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    z = np.linspace(0.25,3.5,10)\n",
    "    ax.text(1.32, -.025, r\"$x_1$\", fontsize=24)\n",
    "    ax.text(-.05, 1.32, r\"$x_2$\", fontsize=24)\n",
    "    plt.xlim([-1.50,1.50])\n",
    "    plt.ylim([-1.50,1.550])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.xticks([], fontsize=16)\n",
    "    plt.yticks([], fontsize=16)\n",
    "    \n",
    "def plotSVM(X, y, clf): \n",
    "    msize=100\n",
    "    mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    ax = fig.add_subplot(111)\n",
    "    colors = [mycolors[\"blue\"] if yi==1 else mycolors[\"red\"] for yi in y]\n",
    "    plt.scatter(X[:,0],X[:,1], marker='o', color=colors, s=msize, alpha=0.25)\n",
    "    plt.arrow(-1.25,0,2.5,0, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    plt.arrow(0,-1.25,0,2.5, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    z = np.linspace(0.25,3.5,10)\n",
    "    ax.text(1.32, -.025, r\"$x_1$\", fontsize=24)\n",
    "    ax.text(-.05, 1.32, r\"$x_2$\", fontsize=24)\n",
    "    plt.xlim([-1.50,1.50])\n",
    "    plt.ylim([-1.50,1.50])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.xticks([], fontsize=16)\n",
    "    plt.yticks([], fontsize=16)\n",
    "    \n",
    "    clf.fit(X,y)\n",
    "\n",
    "    x_min = X[:, 0].min()+.00\n",
    "    x_max = X[:, 0].max()-.00\n",
    "    y_min = X[:, 1].min()+.00\n",
    "    y_max = X[:, 1].max()-.00\n",
    "\n",
    "    colors = [mycolors[\"blue\"] if yi==1 else mycolors[\"red\"] for yi in y]\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.contour(XX, YY, Z, colors=[mycolors[\"red\"], \"gray\", mycolors[\"blue\"]], linestyles=['--', '-', '--'],\n",
    "                levels=[-1.0, 0, 1.0], linewidths=[2,2,2], alpha=0.9)\n",
    "    \n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "    \n",
    "def plotGrid(grid):\n",
    "    \n",
    "    scores = [x[1] for x in grid.grid_scores_]\n",
    "    scores = np.array(scores).reshape(len(crng), len(grng))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n",
    "               norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n",
    "    plt.xlabel('gamma')\n",
    "    plt.ylabel('C')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(grng)), grng, rotation=45)\n",
    "    plt.yticks(np.arange(len(crng)), crng)\n",
    "    plt.title('Validation accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
