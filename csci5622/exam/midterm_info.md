Midterm Information 
=

**Time**/**Date**: 19. October at 4:30-5:45pm in HUMN 135  

Overview and Rules  
--------
- The exam covers everything we have done in class up to and including multi-class classification and error correcting output codes (but not ranking).  This includes all material presented in videos, in-class discussion, in-class exercises, and material introduced in homework. 
- You are allowed one 8-1/2 x 11in sheet of **handwritten** notes (both sides).  No magnifying glasses! 
- You may use a calculator provided that it cannot access the internet or store large amounts of data. 
- The exam will be a mixture of multiple choice questions and free-response questions in which you may be asked to work through simple examples of algorithms or proofs. 




Material Overview 
---

**General**
- differences between classification and regression 
- differences between supervised and unsupervised learning 
- basic probability 

**K-Nearest Neighbors**
- how the algorithm works 
- how to perform classification with the algorithm 
- properties of the algorithm 

**Naive Bayes**
- assumptions behind the algorithm 
- how to compute probabilities from training data 
- how to perform classification with the algorithm 
- Laplace smoothing and it's variants 

**Logistic Regression**
- assumptions behind the algorithm 
- its probabilistic interpretation 
- how to perform classification with the algorithm 
- how to find the weights using Stochastic Gradient Ascent 
- how to efficiently perform regularization in the context of SGA 

**Support Vector Machines** 
- the concept of a margin 
- the geometry behind SVM 
- the workings of hard-margin SVM 
- the workings of soft-margin SVM 
- what a support vector is 
- the SMO algorithm for determining the model 
- ideas related to different kernels 

**Decision Trees** 
- general properties of the algorithm 
- entropy, impurity, and information gain 
- how to choose the best split 

**Boosting**
- properties of the AdaBoost algorithm 
- the concept of a weak learner 
- how weights are calculated in AdaBoost 
- how predictions are made from an AdaBoost model 

**Regression**
- probabilistic interpretation of regression models 
- generalities of how weights are estimated 
- interpretation of the weights in regression models 

**Regularization** 
- the point of regularization 
- Ridge (L2) Regression 
- LASSO (L1) Regression 
- Ridge vs LASSO regression 
- the effect on bias/variance 

**Learning Theory**
- the Bias/Variance Trade-Off
- overfitting, generalization, etc 
- finite vs infinite hypothesis classes 
- PAC Learnability: what it means, what is involved, simple bounds (but no proofs) 
- VC Dimension: how to compute it (possibly simple proofs), why it matters 

**Multi-class Classification**
- One-vs-All classification 
- All-Pairs classification 
- Error Correcting Output Codes 


